                 COMP 40 Lab: Striding Through Memory
        (for groups of two -- work with your locality partner)



+--------------------------------------------------------+
|Keeper of the record:            Alex Violet (aviole01) |
|--------------------------------------------------------|
|Lab partner:                 Sabeeh Iftikhar (siftik01) |
+--------------------------------------------------------+


Please edit this document to include your answers to the questions
below, and use the submit40-lab-strides script to submit your
answers. 

Read these questions before you start doing the lab experiments, and
use these questions to guide your choice of test cases. Remember, the
particular tests listed in the instructions are just hints for getting
you started: you should run any experiments that you think will help
you answer these questions or understand how the cache works.

Don't worry if you aren't sure of an answer to a given quesetion.
The goal here is to start teaching you to do what cache
designers do: think step-by-step through what happens in a cache as
a program runs, use actual simulations to determine which designs
perform best on which applications, and extract general
principles of cache design from the results of these simulations.

FOR THESE FIRST FEW QUESTIONS, ASSUME A DIRECT MAPPED CACHE (the
-assoc 1 setting for testcachesim, which is the default).

Cache Size
----------

Q1a: If you know the block size in bytes for a cache and the number of
     lines in the cache, what will be the total size of the cache in
     bytes? 

# of lines * size of each line (block)

Q1b: For testcachesim, describe in detail how performance changes as
     the size of the cache gets larger, presuming the size of the
     test array remains the same?  

     Performance improves since higher hit rate and less misses. 

Q1c. Is there a point beyond which performance stops improving as
     cache size increases? Why?

     Yes, once the cache can hold all the frequently-accessed data increasing
     cache size more won't improve performance much more. 

Q1d. Sometimes performance is excellent (that is, the cache gives us a
     very good speed up) but then making the test array just a little
     bigger reduces performance dramatically. Why?

     When array > what cache holds, we get capacity messes. When working set > 
     cache capacity, we get a lot of cache misses again. 

Block sizes
-----------

In this section, assume that the total size of the cache we can build
is fixed, but that we get to make choices as to whether we have
fewer lines with bigger blocks, or more lines with smaller blocks.

Q2.  Are bigger blocks always better? Worse? Assuming the total size
     of the cache remains the same, and for an application like
     testcachesim, which block size is best?

     Pros: Better spatial locality
     Cons: Wasted data if u don't access nearby elements

     For testcachesim larger blocks are probably better since ur going to access
     nearby elements. 

Writing data
------------

Q3.  Reread the part of the instructions that explains the
     "Reads_for_writes" count in your cache statistics. Is there a
     value of the block size that will make "Reads_for_writes" zero?
     If you understand this, then you understand a lot about how
     "write-back" caches work.

     Yes. Reads_for_writes happen when writing to cahche line not already in 
     cache so cache first reads entire block from memory before write. 
     
     But If block size = size of data element and writing one element at a time
     u could avoid it. I dont think this would happen often since cache blocks
     are usually larger than individual data elements.

Q4.  Explain why, for applications that update memory repeatedly, a cache that
     performs better may finish with more dirty blocks than a cache
     that does not perform well on the same application.

     Bc its keeping frequently accessed data in the cache. Poor cache will evict
     quicker so less chance for multiple writes on the same cache line, whereas
     an efficient cache will have higher chance for writes since it's in the
     cache for l;onger (thus more dirty blocks)

=================================================================
                     Associative caches
=================================================================

Q5.  Can you describe a situation in which a fully associative cache
     will outperform a direct-mapped cache?

     When there are conflicts in direct-mapped cache (conflict misses)
     If access pattern has bad spatial locality
     If you have small cache w large working set

Submit this file using script

       submit40-lab-strides
